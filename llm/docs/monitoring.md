# 监控与可观测性

LLM系统的监控与可观测性是确保服务质量、故障快速检测和持续优化的关键。本文档介绍构建全面监控系统的最佳实践。

## 监控金字塔

LLM系统监控应采用分层方法:

```
                    ┌───────────────┐
                    │  业务指标     │
                    │ (用户体验/ROI) │
                    └───────┬───────┘
                            │
                    ┌───────┴───────┐
                    │  应用指标     │
                    │ (质量/性能)   │
                    └───────┬───────┘
                            │
                    ┌───────┴───────┐
                    │  资源指标     │
                    │ (CPU/GPU/内存) │
                    └───────────────┘
```

## 核心监控维度

### 1. 性能监控
- **延迟指标**:
  - 端到端响应时间
  - 首字生成时间(TTFT)
  - 每秒生成Token数(TPS)
- **吞吐量指标**:
  - 每秒请求数(RPS)
  - 每分钟处理Token数
  - 批处理效率
- **资源利用率**:
  - GPU计算利用率
  - GPU内存使用
  - CPU使用率
  - 系统内存使用

### 2. 可靠性监控
- **可用性指标**:
  - 服务可用时间百分比
  - 错误率
  - 成功请求百分比
- **故障检测**:
  - 异常响应时间
  - 错误类型分布
  - 重试次数
- **饱和度指标**:
  - 队列长度
  - 队列等待时间
  - 拒绝请求数

### 3. 质量监控
- **模型质量**:
  - 生成内容质量评分
  - 幻觉检测率
  - 自动评估指标(BLEU等)
- **用户体验**:
  - 用户反馈评分
  - 任务完成率
  - 会话连贯性

### 4. 成本监控
- **资源成本**:
  - 每1,000 tokens成本
  - 每次请求成本
  - GPU小时用量
- **效率指标**:
  - 成本与质量比率
  - 缓存命中率
  - 资源利用效率

## 可观测性数据收集

### 日志收集
- **结构化日志**: JSON格式便于解析
- **关键日志字段**:
  - 请求ID
  - 模型信息
  - 输入/输出长度
  - 处理时间
  - 错误信息
  - 资源使用情况
- **日志级别策略**: 不同环境的日志级别控制

### 指标收集
- **系统指标**: 使用Prometheus/Grafana收集
- **应用指标**: 自定义应用指标导出
- **聚合策略**: 
  - 时间窗口聚合(1m, 5m, 1h)
  - 分位数指标(p50, p95, p99)

### 追踪采集
- **分布式追踪**: 使用OpenTelemetry/Jaeger
- **追踪范围**:
  - 请求预处理
  - 模型推理阶段
  - 后处理步骤
- **采样策略**: 基于请求特征的智能采样

## 监控系统架构

### 典型监控架构
```
┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐
│ 数据收集 │ -> │ 数据处理 │ -> │ 数据存储 │ -> │ 数据可视化│
└──────────┘    └──────────┘    └──────────┘    └──────────┘
     │               │               │               │
     v               v               v               v
┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐
│Prometheus│    │ Kafka    │    │InfluxDB/ │    │ Grafana  │
│OpenTele. │    │ Fluentd  │    │Elasticsearch    │ Kibana   │
└──────────┘    └──────────┘    └──────────┘    └──────────┘
```

### 推荐监控工具栈

| 监控需求 | 开源工具 | 商业/云服务 |
|---------|---------|------------|
| 指标收集 | Prometheus | Datadog, CloudWatch |
| 日志管理 | ELK Stack | Splunk, Loggly |
| 追踪系统 | Jaeger, Zipkin | New Relic, Dynatrace |
| 可视化 | Grafana | Datadog, Tableau |
| 告警 | Alertmanager | PagerDuty, OpsGenie |

## 告警策略

### 告警设计原则
- **可操作性**: 每个告警都应指向明确的操作
- **噪声控制**: 避免告警风暴和疲劳
- **严重性分级**: 基于业务影响设置不同级别
- **上下文丰富**: 提供足够信息以快速诊断

### 多级告警体系
- **P0 (紧急)**: 服务完全不可用，立即响应
- **P1 (高)**: 重要功能降级，24小时内响应
- **P2 (中)**: 非关键问题，72小时内响应
- **P3 (低)**: 需要关注但不紧急的问题

### 关键告警示例

| 告警名称 | 触发条件 | 严重性 | 响应操作 |
|---------|--------|--------|---------|
| 服务可用性低 | 成功率<99% | P0 | 检查服务健康，回滚部署 |
| 推理延迟高 | P95延迟>阈值 | P1 | 检查资源使用，扩容 |
| GPU利用率低 | 利用率<30% | P2 | 优化批处理，考虑缩容 |
| 幻觉率高 | 幻觉检测>阈值 | P1 | 检查模型/数据问题 |

## 异常检测

### 检测方法
- **统计方法**: 基于历史模式的阈值
- **机器学习方法**: 自动学习正常模式
- **规则引擎**: 预定义业务规则检测

### 常见异常模式
- **突刺(Spike)**: 突然的短暂性能下降
- **趋势(Trend)**: 渐进性能下降
- **级别变化(Level shift)**: 性能永久变化
- **季节性变化(Seasonal)**: 周期性变化

## 性能剖析与调优

### 剖析目标
- **推理热点**: 找出计算瓶颈
- **内存使用**: 识别内存泄漏/低效
- **I/O瓶颈**: 发现存储访问问题

### 剖析工具
- **NVIDIA Nsight**: GPU性能分析
- **PyTorch Profiler**: 深度学习框架性能分析
- **Brendan Gregg工具**: 系统级性能分析

## 监控自动化与MLOps集成

### 自愈系统
- **自动扩缩容**: 基于指标自动调整资源
- **自动重启**: 检测到故障时重启服务
- **自动回滚**: 检测到版本问题时回滚
- **故障迁移**: 将流量从故障节点转移

### MLOps集成
- **模型监控**: 跟踪生产中的模型性能
- **模型漂移检测**: 识别模型随时间变化
- **A/B测试**: 比较不同模型/参数效果
- **实验追踪**: 记录所有实验结果

## 仪表盘与可视化

### 关键仪表盘

- **运维仪表盘**: 系统健康与资源使用
- **性能仪表盘**: 延迟、吞吐量等指标
- **质量仪表盘**: 模型输出质量评估
- **成本仪表盘**: 资源使用和成本追踪

### 示例:LLM服务运维仪表盘
```
┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐
│  服务状态       │ │  请求/分钟      │ │  平均延迟       │
│  ●  正常运行    │ │     ┌───┐       │ │     ┌───┐       │
│                 │ │     │   │       │ │     │   │       │
└─────────────────┘ └─────┴───┴───────┘ └─────┴───┴───────┘

┌──────────────────────────────────────┐ ┌─────────────────┐
│  GPU使用率                           │ │  错误率         │
│                                      │ │                 │
│  ┌─────────────────────────────────┐ │ │     ┌───┐       │
│  │                                 │ │ │     │   │       │
│  └─────────────────────────────────┘ │ │     └───┘       │
└──────────────────────────────────────┘ └─────────────────┘

┌─────────────────────┐ ┌─────────────────────────────────┐
│  模型调用分布       │ │  Top 5 错误类型                 │
│  ┌───┐ ┌───┐ ┌───┐  │ │  1. 超时错误       35%          │
│  │   │ │   │ │   │  │ │  2. 输入验证失败   25%          │
│  └───┘ └───┘ └───┘  │ │  3. 资源耗尽       20%          │
└─────────────────────┘ └─────────────────────────────────┘
```

## 案例研究:生产环境监控系统

### 案例1:大规模LLM服务监控
- **挑战**: 追踪数百个推理工作节点
- **解决方案**: 分层监控架构，集中日志，自动异常检测
- **成果**: 故障检测时间从小时级缩短到分钟级

### 案例2:成本优化监控
- **挑战**: 控制高并发场景下的成本
- **解决方案**: 精细资源监控，自动资源回收
- **成果**: 降低30%运营成本同时维持性能 